{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXhKdIISDr-H"
      },
      "outputs": [],
      "source": [
        "1.What is Simple Linear Regression\n",
        "Ans - Simple Linear Regression is a statistical method used to model the relationship between two variables by fitting a linear equation to observed data.\n",
        " One variable is considered to be an independent variable (predictor), and the other is the dependent variable (response)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression\n",
        "Ans - 1. Linearity-The relationship between the input (X) and output (Y) should be straight-line.\n",
        "    2. Independence- The data points should be independent of each other.\n",
        "    3. Homoscedasticity (Constant Variance)- The spread of errors (difference between actual and predicted values) should be roughly the same across all values of X.\n",
        "    4. Normality of Errors- The errors (residuals) should be normally distributed (bell-shaped curve).\n",
        "    5. No (or little) Multicollinearity- Since Simple Linear Regression only uses one independent variable, this doesn‚Äôt apply here. But in case of more variables, they shouldn‚Äôt be too similar to each other."
      ],
      "metadata": {
        "id": "DXz2wnc3EDOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c\n",
        "Ans - The coefficient m represents the slope of the line.\n",
        "* It tells us how much Y changes for every one unit increase in X.\n",
        "*  m shows the rate of change of the dependent variable (Y) with respect to the independent variable (X).\n"
      ],
      "metadata": {
        "id": "SS7cvBXtE_am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c\n",
        "Ans - The term c is called the intercept (or Y-intercept).\n",
        "* It represents the starting value or baseline of Y when the independent variable X is zero.\n",
        "\n",
        "* It helps define the position of the line vertically on the graph.\n"
      ],
      "metadata": {
        "id": "5aXRU49qFY0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression\n",
        "Ans -"
      ],
      "metadata": {
        "id": "IaUThvArFrfv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48174e41"
      },
      "source": [
        "To calculate the slope (m) in Simple Linear Regression, we use the following formula:\n",
        "\n",
        "$ m = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sum{(X_i - \\bar{X})^2}} $\n",
        "\n",
        "Where:\n",
        "*   $X_i$ and $Y_i$ are the individual data points for the independent and dependent variables, respectively.\n",
        "*   $\\bar{X}$ and $\\bar{Y}$ are the means of the independent and dependent variables, respectively.\n",
        "*   $\\sum$ represents the sum of the values."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression\n",
        "Ans - The Least Squares Method is used to find the best-fitting straight line through the data points in a scatter plot.\n",
        "To minimize the total error between the actual values and the predicted values from the regression line.\n",
        "\n"
      ],
      "metadata": {
        "id": "JpbCQuX-GLKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R¬≤) interpreted in Simple Linear Regression\n",
        "Ans -"
      ],
      "metadata": {
        "id": "FBRVNA7iGYpv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55db5fe4"
      },
      "source": [
        "The coefficient of determination ($R^2$) is a statistical measure that represents the proportion of the variance in the dependent variable (Y) that is predictable from the independent variable (X).\n",
        "\n",
        "Here's how to interpret it:\n",
        "\n",
        "*   **Value Range:** $R^2$ values range from 0 to 1 (or 0% to 100%).\n",
        "*   **Interpretation:**\n",
        "    *   An $R^2$ of 0 means that the independent variable (X) does not explain any of the variability in the dependent variable (Y). The regression model does not fit the data at all.\n",
        "    *   An $R^2$ of 1 means that the independent variable (X) explains all of the variability in the dependent variable (Y). The regression model perfectly fits the data.\n",
        "    *   An $R^2$ value between 0 and 1 indicates the percentage of the variance in Y that is explained by X. For example, an $R^2$ of 0.75 means that 75% of the variation in Y can be explained by the linear relationship with X. The remaining 25% is unexplained by the model.\n",
        "*   **Goodness of Fit:** $R^2$ is often used as a measure of the \"goodness of fit\" of the regression model. A higher $R^2$ generally indicates a better fit of the model to the data.\n",
        "\n",
        "It's important to note that a high $R^2$ does not necessarily mean that the model is good or that the relationship is causal. It only indicates how well the variance in the dependent variable is explained by the independent variable in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression\n",
        "Ans - Multiple Linear Regression (MLR) is an extension of Simple Linear Regression. It models the relationship between one dependent variable and two or more independent variables.\n",
        "It helps us understand how multiple factors (predictors) affect an outcome and allows us to predict the value of the dependent variable based on the values of several independent variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sy1nD3_jGitg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression\n",
        "Ans- The main difference between Simple and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable. Simple Linear Regression involves only one independent variable, and it models the relationship between this single predictor and the outcome using a straight line. In contrast, Multiple Linear Regression involves two or more independent variables, allowing it to capture more complex relationships by considering the combined effect of several predictors on the outcome. While both aim to predict a continuous dependent variable, Multiple Linear Regression provides a more comprehensive model when multiple factors influence the target, whereas Simple Linear Regression is limited to analyzing one-factor influence at a time.\n"
      ],
      "metadata": {
        "id": "tXF4oB7UGwsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression\n",
        "Ans -* Linearity-\tRelationship between predictors and output is linear\n",
        "     * Independence - Residuals (errors) are not related\n",
        "     * Homoscedasticity\t- Errors have constant variance\n",
        "     * Normality - Errors follow a normal distribution\n"
      ],
      "metadata": {
        "id": "R4iTXp1SHOyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "Ans - Heteroscedasticity occurs when the variance of the errors (residuals) in a regression model is not constant across all levels of the independent variables.\n",
        "It means that the spread of prediction errors increases or decreases as the value of an independent variable changes ‚Äî like a fan-shaped pattern in a scatter plot of residuals.\n",
        "\n"
      ],
      "metadata": {
        "id": "g84S9JmrH-A3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "Ans - Multicollinearity occurs when two or more independent variables are highly correlated with each other. This can cause unstable coefficient estimates, make interpretation difficult, and inflate standard errors, reducing the reliability of hypothesis tests.\n"
      ],
      "metadata": {
        "id": "3bVoz8IiIQwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models\n",
        "Ans - Categorical variables contain labels or names  that need to be converted into numerical form before they can be used in regression models like Linear Regression or Logistic Regression.\n",
        "  *  One-Hot Encoding\n",
        "  *  Label Encoding"
      ],
      "metadata": {
        "id": "LtaTrFOyIgBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression\n",
        "Ans - An interaction term is created by multiplying two (or more) independent variables together. It helps the model account for the combined effect of those variables, which might not be captured by their individual contributions alone\n",
        "Interaction terms in Multiple Linear Regression allow the model to capture situations where the effect of one independent variable on the dependent variable depends on the value of another variable.\n",
        "\n"
      ],
      "metadata": {
        "id": "iY3nEr_cI6dC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "Ans - * In Simple Linear Regression, the intercept represents the expected value of the dependent variable when the independent variable is equal to zero. It has a straightforward interpretation because there is only one predictor. For example, in a model predicting salary based on years of experience, the intercept tells us the predicted salary of someone with zero years of experience. In this case, the intercept often has a clear and meaningful real-world interpretation.\n",
        "\n",
        "* In Multiple Linear Regression, however, the interpretation of the intercept becomes more complex. It still represents the predicted value of the dependent variable when all independent variables are equal to zero, but this situation is often unrealistic or nonsensical in real life. For example, if you're predicting house price based on size, location, and age, the intercept would represent the predicted price when size, location, and age are all zero ‚Äî a scenario that doesn‚Äôt make practical sense. Therefore, in Multiple Linear Regression, the intercept is usually less meaningful on its own, and analysts focus more on the coefficients of the independent variables."
      ],
      "metadata": {
        "id": "eU2M31eLJJy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "Ans - The slope in regression analysis is one of the most important components of the model. It represents the rate of change in the dependent variable (Y) with respect to a one-unit change in an independent variable (X), while keeping all other variables constant.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qj-G74_kJbIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables\n",
        "Ans - The intercept in a regression model plays a crucial role in establishing the starting point or baseline value of the dependent variable (Y) when all the independent variables (X‚ÇÅ, X‚ÇÇ, ..., X‚Çô) are equal to zero. While it may not always be meaningful in practical terms, it provides important context for interpreting the overall relationship between variables"
      ],
      "metadata": {
        "id": "H-lEfFDWJlAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R¬≤ as a sole measure of model performance\n",
        "Ans - R¬≤ (coefficient of determination) measures how well the independent variables explain the variation in the dependent variable. While it‚Äôs a popular metric, relying on R¬≤ alone can be misleading.\n",
        "*  R¬≤ Always Increases When More Variables Are Added\n",
        "*  Does Not Indicate Model Accuracy\n",
        "*  Cannot Detect Bias or Overfitting\n",
        "*  Not Useful for Comparing Models with Different Dependent Variables\n",
        "*  Insensitive to the Scale of Errors"
      ],
      "metadata": {
        "id": "xDwV_f7bJutm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient\n",
        "Ans - In regression analysis, the standard error of a coefficient measures how precisely that coefficient is estimated. A large standard error means the estimate of the coefficient is not reliable and may vary significantly across samples.\n",
        "\n"
      ],
      "metadata": {
        "id": "puftJXiEKKrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "Ans - * Identifying Heteroscedasticity in Residual Plots\n",
        "A residual plot displays the residuals (errors) on the y-axis and the predicted values or an independent variable on the x-axis.\n",
        "\n",
        "In an ideal regression model (with homoscedasticity), the residuals should appear randomly scattered around zero with constant variance.\n",
        "\n",
        "* Why Is It Important to Address Heteroscedasticity?\n",
        "Even though the regression coefficients themselves remain unbiased, heteroscedasticity affects other crucial aspects of the model"
      ],
      "metadata": {
        "id": "nvPxoADBKUJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 21.  What does it mean if a Multiple Linear Regression model has a high R¬≤ but low adjusted R¬≤\n",
        "Ans - When a regression model has a high R¬≤ but a low adjusted R¬≤, it often indicates that the model includes irrelevant or unnecessary predictors that do not contribute meaningfully to explaining the dependent variable.\n",
        "* What R¬≤ and Adjusted R¬≤ Mean:\n",
        "R¬≤ (Coefficient of Determination):\n",
        "Measures the proportion of variance in the dependent variable explained by the independent variables.\n",
        "Always increases or stays the same when more variables are added ‚Äî even if they're not useful.\n",
        "* Adjusted R¬≤:\n",
        "Adjusts R¬≤ by penalizing the model for adding predictors that don‚Äôt improve model performance.\n",
        "Can decrease if new variables add noise instead of value.\n",
        "\n"
      ],
      "metadata": {
        "id": "7lXTVGjQKriR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression\n",
        "Ans - Scaling variables ‚Äî such as standardizing (z-score) or normalizing ‚Äî is important in Multiple Linear Regression for both practical and interpretive reasons, especially when the model includes predictors with different units or ranges.\n",
        "*  To Ensure Fair Contribution from All Variables\n",
        "*  Required for Regularization (Ridge, Lasso)\n",
        "*  Improves Numerical Stability and Convergence\n",
        "*  Helps Interpret Relative Importance (When Needed)"
      ],
      "metadata": {
        "id": "5JRJDyTcLEDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression\n",
        "Ans - Polynomial Regression is a type of regression analysis in which the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial. It is an extension of linear regression that allows the model to fit non-linear relationships."
      ],
      "metadata": {
        "id": "EDh71zItLewv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression\n",
        "Ans - Polynomial Regression and Linear Regression are both supervised learning algorithms used for predicting a continuous dependent variable, but they differ in how they model the relationship between the independent and dependent variables.\n",
        "*  1. Model Shape:\n",
        "Linear Regression assumes a straight-line (linear) relationship between the independent variable(s) and the dependent variable.\n",
        "* 2. Nature of the Relationship\n",
        "* 3. Model Complexity:\n",
        "Linear Regression is simpler, easier to interpret, and less prone to overfitting."
      ],
      "metadata": {
        "id": "AQzUgIYhLmav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used\n",
        "Ans - Polynomial Regression is used when the relationship between the independent variable(s) and the dependent variable is non-linear, but can be modeled as a polynomial function.\n",
        "*  Data Shows a Curved Trend:\n",
        "If a straight line doesn't fit the data well, but a curved line (like a parabola or S-curve) does, polynomial regression is a better choice.\n",
        "*  Residuals in Linear Regression Show Patterns:\n",
        "If a residual plot (errors vs. predicted values) shows a systematic pattern (like a curve), that‚Äôs a sign that a linear model is missing curvature. Polynomial terms can fix this.\n",
        "* You Want More Flexibility than a Linear Model Can Provide\n",
        "Polynomial regression can fit a wider range of shapes by adding higher-order terms\n",
        "\n"
      ],
      "metadata": {
        "id": "CU-zkI_NMA38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression\n",
        "Ans - The model is still linear in the parameters (the ùõΩ\n",
        "Œ≤'s), which means it can be estimated using linear regression techniques, even though it‚Äôs non-linear in the predictor variable\n",
        "ùëã\n",
        "X"
      ],
      "metadata": {
        "id": "G_eqRavLMbEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables\n",
        "Ans - Yes, Polynomial Regression Can Be Applied to Multiple Variables\n",
        "This is known as Multivariate Polynomial Regression, where you include polynomial terms not just for one variable, but for two or more independent variables and their interactions."
      ],
      "metadata": {
        "id": "W-H0_ZJSMu4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression\n",
        "Ans - Limitations of Polynomial Regression\n",
        "* Risk of Overfitting:\n",
        "Higher-degree polynomials may fit the training data too well, capturing noise instead of the true pattern, which leads to poor generalization on new data.\n",
        "* Sensitive to Outliers:\n",
        "Outliers can heavily influence the shape of the polynomial curve, leading to inaccurate predictions.\n",
        "* Unstable Extrapolation:\n",
        "Predictions outside the range of the training data (extrapolation) can become extremely large or erratic.\n",
        "* Increased Computational Complexity:\n",
        "For multiple variables and higher degrees, the number of terms grows quickly, increasing the time and resources needed\n",
        "* Requires Careful Degree Selection:\n",
        "Choosing the wrong degree (too low or too high) can lead to underfitting or overfitting. Trial and error or cross-validation is often needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "YzuNlzJZM406"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "Ans - When using polynomial regression, choosing the right degree is crucial to balance underfitting and overfitting. Below are common and effective methods to evaluate model fit and guide degree selection:\n",
        "*  1. Cross-Validation\n",
        "*  2. Mean Squared Error (MSE) or Root Mean Squared Error (RMSE)\n",
        "*  3. R¬≤ and Adjusted R¬≤\n",
        "*  4. Residual Plots\n",
        "*  5. Learning Curves\n",
        "*  6. Information Criteria (AIC/BIC)"
      ],
      "metadata": {
        "id": "E4PNgeBNNlBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression\n",
        "Ans -  visualization is important in polynomial regression because\n",
        "* Shows Curve Fit :\n",
        " Helps see how well the polynomial model captures the data‚Äôs shape.\n",
        "* Detects Underfitting/Overfitting :\n",
        " Visualizes if the model is too simple or too complex.\n",
        "* Aids Degree Selection :\n",
        " Helps compare different polynomial degrees visually.\n",
        "* Checks Residuals :\n",
        " Residual plots reveal patterns or errors in model fitting.\n",
        "* Spots Outliers  :\n",
        "Highlights unusual data points affecting the model.\n",
        "* Improves Communication :\n",
        "Makes results easier to understand and explain"
      ],
      "metadata": {
        "id": "4wAIDYysOB8g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZO26SghOiXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "Ans - Polynomial regression in Python is a process of modeling a non-linear relationship between the independent variable ùëã and the dependent variable ùëå\n",
        "by transforming the input features into polynomial features, and then applying linear regression on the transformed data.\n",
        "\n"
      ],
      "metadata": {
        "id": "AfamDVqePCYt"
      }
    }
  ]
}